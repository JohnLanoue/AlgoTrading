{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d8945c",
   "metadata": {},
   "source": [
    "## Intent 9/6/2025: To create a LSTM vectorized Backtest function that can run with a data frame of predictions.\n",
    "\n",
    "Start with building a LTSM Predictor hence we already have a get data function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fcb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic \n",
    "import numpy as np\n",
    "#LSTM Functions\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from Packages.Alpaca_Connection import get_data_today\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650c3e8",
   "metadata": {},
   "source": [
    "## Seperate: The LSTM Predictions\n",
    "Future Items: \n",
    "    Customize the future_days available.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a51fc",
   "metadata": {},
   "source": [
    "Overhead for the data: Consider this an independant function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_finalize(df, symbol = 'mmm' ):\n",
    "    #PRE: Recieves a data frame from the Alpaca_connection.get_data_today package\n",
    "    #POST: Finalizes a data frame that is ready for next steps. \n",
    "    #Intent: Band-aid for data handling.  Consider closing the gap in get_data_today\n",
    "    #Improvements: allow for time parameters incoming. Replace function ALL together\n",
    "\n",
    "bars = get_data_today(symbol)\n",
    "df= get_data_today(symbol)\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns = {'open':'Open', 'high':'High', 'low':'Low', 'close':'Close', 'volume':'Volume', 'trade_count':'Trade Count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00152436",
   "metadata": {},
   "source": [
    "Below we should encapsulate this into a single function that can prep for LTSM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55c084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Consider making future_days customizable: it is the lookforward period\n",
    "future_days = 38\n",
    "#THis is the target preictor: Conside\n",
    "df['Prediction'] = df[['Close']].shift(-future_days)\n",
    "\n",
    "# In[23]:\n",
    "X = np.array(df.drop(['Prediction','index', 'symbol', 'timestamp'], axis = 1))[:-future_days]\n",
    "y = np.array(df['Prediction'])[:-future_days]\n",
    "print(\"True Values\")\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Finally, we use the prediction column to create an array of the the future predictor days.  \n",
    "# Get the last 'x' rows of the feature data set\n",
    "x_future = df.drop(['index','Prediction', 'symbol', 'timestamp'],axis = 1)[:-future_days]\n",
    "x_future = x_future.tail(future_days)\n",
    "x_future = np.array(x_future)\n",
    "# ## Splitting the data for performance testing\n",
    "# In addition to the visual analysis, we are going to want to use our conventional Test/train split in order to train the data. We can use our conventional train_test_split dividing our 'live' close date with our prediction close date.   \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Builds the actual LTSM Network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484706fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## LSTM network\n",
    "# From background research I have done with predictive stock market modeling it appears that LSTM is the most common method used and I would not be a fool to explore it.  It manages to hold onto important information and 'forget' about useless information hence Long Term Short Memory.  \n",
    "# ### Reprep the data\n",
    "# It appeares that the LSTM Model is much more sensitive to the data structure than the other methods.  After running the same data the model was set on deliving a constant.  It appears that the data has to me scaled and properly wrangled before usage.  Hence we will have to reprepare the data.  \n",
    "\n",
    "# As seen before we are going to look at the market close data and prepare all but 38 values to generate predictions.  \n",
    "\n",
    "lstmData = df.filter(['Close'])\n",
    "dataset = lstmData.values\n",
    "training_data_len = len(dataset)-38\n",
    "\n",
    "\n",
    "# Now as mentioned above, we have to scale the data.  Using the min max scaler we can turn all of our data into decimals (except for the min =0 and max =1).  We will save this data into our scaled_data datset.    \n",
    "\n",
    "#Scale the data \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "# Below you will see that we need to split the data again into x and y train and test.  Using a 60 day offset we can append them to an empty list using the prior 60 days to predict the next 60 days.  \n",
    "\n",
    "#Re create Training Set \n",
    "lstm_train_data = scaled_data[0:training_data_len,:]\n",
    "test_data = scaled_data[training_data_len - 60: ,:]\n",
    "\n",
    "# Create the datasets\n",
    "lstm_x_train = []\n",
    "lstm_y_train = []\n",
    "lstm_x_test = []\n",
    "lstm_y_test = dataset[training_data_len:, :]\n",
    "\n",
    "for i in range(60, len(lstm_train_data)):\n",
    "    lstm_x_train.append(lstm_train_data[i-60:i])\n",
    "    lstm_y_train.append(lstm_train_data[i,0])\n",
    "    \n",
    "\n",
    "for i in range(60, len(test_data)): \n",
    "    lstm_x_test.append(test_data[i-60:i, 0])    \n",
    "\n",
    "\n",
    "# Now we have to convert the lists into numpy arrays so the data can be read.  Also, we have to reshape them into a format that the LSTM will understand.  \n",
    "\n",
    "#Conver to numpy\n",
    "lstm_x_train, lstm_y_train =   np.array(lstm_x_train), np.array(lstm_y_train)\n",
    "lstm_x_test = np.array(lstm_x_test)\n",
    "#Reshape the data\n",
    "lstm_x_train = np.reshape(lstm_x_train, (lstm_x_train.shape[0], lstm_x_train.shape[1], 1))\n",
    "lstm_x_test = np.reshape(lstm_x_test, (lstm_x_test.shape[0], lstm_x_test.shape[1], 1))\n",
    "\n",
    "\n",
    "# Finally, we are back to building the models.  We are using 4 layers: The first two training layers then a dense layer and finally an output layer.  \n",
    "model = Sequential() \n",
    "model.add(LSTM(50, return_sequences = True, input_shape=(lstm_x_train.shape[1],1)))\n",
    "model.add(LSTM(50, return_sequences = False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "# Now that we have created the network, we can run the data through using the MSE method.  We will use 10 epocs because the loss is low, but futher iterations are not necessary.  Finally we have to undo the scaling to our predictions.  \n",
    "\n",
    "#Compile the model \n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#Train the model \n",
    "model.fit(lstm_x_train, lstm_y_train, batch_size = 1, epochs = 10)  \n",
    "LSTM_predictions = model.predict(lstm_x_test)\n",
    "LSTM_predictions = scaler.inverse_transform(LSTM_predictions)\n",
    "LSTM_predictions\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "# Because of the necessary transformations that were done to the model, we cannot use the same function as above. However we can poach  a massive chunk from above and paste it into our model.  \n",
    "\n",
    "t = lstmData[:training_data_len]\n",
    "v = lstmData[training_data_len:]\n",
    "v['Predictions'] = LSTM_predictions\n",
    "\n",
    "\n",
    "\n",
    "#mean_squared_error(X_test[:38],LSTM_predictions)\n",
    "\n",
    "print(\"Actual Values:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48eab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we have the actual buy sell strategy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test[0][2]\n",
    "#df.head()\n",
    "X_test[0]\n",
    "\n",
    "    \n",
    "[row[2] for row in X_test]\n",
    "\n",
    "\n",
    "# ## Buy/Sell Strategy: \n",
    "# If the next prediction is less than current actual: Sell\n",
    "# If the next prediction is more than the current actual: Buy\n",
    "# Create a dataframe of the values with the buy/sell signal.  \n",
    "# Then create look into backtesting the signal.  \n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "buy_sell = pd.DataFrame({\"Actuals\": [row[2] for row in X_test][15:]})#, \"Predictions\":LSTM_predictions })\n",
    "buy_sell[\"Predictions\"] = LSTM_predictions\n",
    "buy_sell[\"Predictions Offset\"] = pd.DataFrame(LSTM_predictions).shift(1)\n",
    "buy_sell[\"signal\"] = np.where(buy_sell[\"Actuals\"]> buy_sell[\"Predictions Offset\"], 1, -1 )\n",
    "buy_sell[\"returns\"] = np.log(buy_sell[\"Actuals\"]/ buy_sell[\"Actuals\"].shift(1))\n",
    "buy_sell[\"strategy\"] = buy_sell[\"returns\"]*buy_sell[\"signal\"]\n",
    "buy_sell\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "buy_sell[[\"returns\", \"strategy\"]].sum()\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "buy_sell[[\"returns\", \"strategy\"]].sum().apply(np.exp)\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "buy_sell[[\"returns\", \"strategy\"]].cumsum().apply(np.exp).plot()\n",
    "\n",
    "\n",
    "# ## Average period return risk statistics for both the stock and strategy. \n",
    "# (32 = # of cycles; 252 = annual period)\n",
    "# \n",
    "# Calculates the period mean return in both log and regular price\n",
    "#  \n",
    "# Conclusion(8/30): Equal risk, much higher reward\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "buy_sell[['returns', 'strategy']].mean()*32\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "np.exp(buy_sell[['returns', 'strategy']].mean()*32)-1\n",
    "\n",
    "\n",
    "# Calculates the period standard deviation of both log and regular spaces\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "buy_sell[['returns', 'strategy']].std()*32**.5\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "(buy_sell[['returns', 'strategy']].apply(np.exp)-1).std()*32**.5\n",
    "\n",
    "\n",
    "# ## Evaluate the drawdaown with the cummax/cumret\n",
    "# \n",
    "# Defines a new column, cumret, with a gross performance over time\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "## Evaluate the drawdaown with the cummax/cumret \n",
    "buy_sell['cumret'] =buy_sell['strategy'].cumsum().apply(np.exp)\n",
    "\n",
    "\n",
    "# Defines yet another column with a running maximum value of the gross performance\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "buy_sell['cummax'] = buy_sell['cumret'].cummax()\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "buy_sell[['cumret', 'cummax']].dropna().plot(figsize= (10,6))\n",
    "\n",
    "\n",
    "# The max drawdown is calcualated as the difference between the two columns\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "drawdown = buy_sell['cummax'] - buy_sell['cumret']\n",
    "drawdown.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed139fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRE: Accepts a pandas data frame with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12579083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
