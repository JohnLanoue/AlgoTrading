{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9a97ad",
   "metadata": {},
   "source": [
    "## Dependant libraries: \n",
    "Below, we import all of the libraries in scope.  This project will require getting data from yahoo finance articles and ticker data.  These will require seperate apis for pulling the data independatly.  Further into the project, we will need to be leveraging the NLP functions as well as machine learning fitting and evaulation packages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e6bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johnlanoue/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johnlanoue/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johnlanoue/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2025-09-06 17:10:09.956837: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Given Data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Web Scraping Support\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta, date, datetime\n",
    "#NLP Essentials\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#NLP Additions\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Basic ML Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "#Finance\n",
    "import yfinance as yf\n",
    "#MISC\n",
    "import asyncio\n",
    "#import config\n",
    "#from alpaca.trading.client import TradingClient\n",
    "#import alpaca_trade_api as api\n",
    "#from alpaca.trading.requests import MarketOrderRequest\n",
    "#from alpaca.trading.enums import OrderSide, TimeInForce\n",
    "#import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ebfc1",
   "metadata": {},
   "source": [
    "## Aquiring Data\n",
    "Below we be looking for articles that are included for a specific symbol.  These articles will be scraped using beautiful soup then inserted into tokens.  A date will be extracted in order to look up the the ticker data. Unfortuatly, the number of articles that may be requested without risking a dropped conection is limited. We will be targeting following Medicare giants: United Health Group, Humana, CVS, Cigna, Evolent, Molina, Clover Health.  The intent of limiting the project to a single topic is that vocabulary can vary from industry to industry.  An example is that an article that includes \"Medicare Advantage\" should be neutral, while most indutries would recognnize \".... Advantage\" as positive.  Therefore, I would like to train them independantly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c48c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yahoo_finance_news(symbol = 'UNH'):\n",
    "    # URL of Yahoo Finance's news page\n",
    "#    url = 'https://finance.yahoo.com/news/'\n",
    "#    url = 'https://finance.yahoo.com/quote/UNH?.tsrc=fin-srch'\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    url = 'https://finance.yahoo.com/quote/'+symbol+'/?.tsrc=fin-srch'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all news article headlines and links\n",
    "    headlines = soup.find_all('h3', class_='Mb(5px)')\n",
    "\n",
    "    # Extract headlines and links\n",
    "    news_data = []\n",
    "    for headline in headlines:\n",
    "        title = headline.get_text()\n",
    "        link = headline.find('a')['href']\n",
    "        news_data.append({'title': title, 'link': link, 'Symbol': symbol})\n",
    "\n",
    "    return news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4e4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_article(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main article content\n",
    "        article_content = soup.find('div', class_='caas-body')\n",
    "        if article_content is None:\n",
    "            # If the article content is not found under 'caas-body', try 'cass-body-content'\n",
    "            article_content = soup.find('div', class_='caas-body-content')\n",
    "\n",
    "        if article_content:\n",
    "            # Extract text from the article content\n",
    "            article_text = article_content.get_text()\n",
    "\n",
    "            # Tokenize the words\n",
    "            tokens = word_tokenize(article_text)\n",
    "\n",
    "            return tokens\n",
    "        else:\n",
    "            return \"Article content not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f60a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def medicare_validation(tokens): \n",
    "    for i in tokens: \n",
    "        if i  == \"Medicare\":\n",
    "            return True \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a8e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_articles(chunk): \n",
    "    #PRE: Adding a list object that includes the news title (technically optional) and the link[suffix] to the article)\n",
    "    #POST: Each element of the list includes the article in the dictionary object\n",
    "    for i in chunk: \n",
    "        i['Article'] = tokenize_article('https://finance.yahoo.com'+ i['link'])\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a27e7",
   "metadata": {},
   "source": [
    "## Gathering the data\n",
    "These steps below will be leveraging the functions above that will create an list of dictionaries object, based on a single stock symbol (or keyword technically).\n",
    "### Below we are showing the  titles of the articles and the path beyond the yahoo finance root\n",
    "A small sample of title, link, symbols are included below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c1d155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yahoo_finance_news=[]\n",
    "#topics = ['UNH','HUM', 'CVS', 'CI', 'ELV', 'MOH', 'CLOV']\n",
    "topics = ['AAPL',\t'GOOGL',\t'MSFT',\t'AMZN',\t'FB',\t'TSLA',\t'JPM',\t'JNJ',\t'NVDA',\t'V',\t'BAC',\t'WMT',\t'PG',\t'MA',\t'DIS',\t'HD',\t'INTC',\t'UNH',\t'VZ',\t'CRM',\t'NFLX',\t'PYPL',\t'CMCSA',\t'ORCL',\t'NKE',\t'CSCO',\t'PFE',\t'XOM',\t'T',\t'ADBE',\t'CVX',\t'MRK',\t'KO',\t'WFC',\t'PEP',\t'ABT',\t'IBM',\t'BA',\t'COST',\t'MCD',\t'SBUX',\t'BMY',\t'LLY',\t'MMM']#,\t'AAP',\t'HON']#,\t'TXN',\t'AVGO',\t'GM',\t'C',\t'LMT',\t'TMO',\t'MDLZ',\t'UPS',\t'CHTR',\t'DHR',\t'SBAC',\t'NOW',\t'ACN',\t'AMGN',\t'CAT',\t'ABBV',\t'CME',\t'NEE',\t'AXP',\t'SPGI',\t'MDT',\t'GS',\t'LIN',\t'FIS',\t'GE',\t'KMB',\t'GILD',\t'FDX',\t'CVS',\t'DUK',\t'WBA',\t'SO',\t'ISRG',\t'BKNG',\t'CRM',\t'ECL',\t'NEE',\t'DD',\t'NSC',\t'TGT',\t'DUK',\t'LOW',\t'MS',\t'RTX']\n",
    "for i in topics: \n",
    "    yahoo_finance_news.append(scrape_yahoo_finance_news(i))\n",
    "yahoo_finance_news = [item for sublist in yahoo_finance_news for item in sublist]    \n",
    "yahoo_finance_news[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09ef45",
   "metadata": {},
   "source": [
    "### Now we are creating a new object, like above including the article content\n",
    "This object will temporarily hold all of the articles.  The articles will be formatted in tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6e1e5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3r/pc8xl1ls1y35f9xzjdj87z0c0000gn/T/ipykernel_68335/3004440463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myahoo_finance_news\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred = add_articles(yahoo_finance_news)\n",
    "predtmp = pred\n",
    "pred[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a677131",
   "metadata": {},
   "source": [
    "## Handle the text\n",
    "As you can see the incoming data from the article is still pretty raw.  We should preprocess the text for sentiment analysis: \n",
    "    Handling the case, so that puncuation patterns do not generate their own noisy indicators\n",
    "    Eliminating stopowords, in order to be ignore the evaluation of meaninless words\n",
    "    Stemmer: We will be using porter.  This will unite a lot of different words by eliminating their participals.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def pre_process_text(tokens): \n",
    "    #Case Handle\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    #Stopword handle\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = [token for token in tokens if token not in stop_words]\n",
    "    #Remove Punctuation\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    #Stem words: Porter\n",
    "#    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7c418",
   "metadata": {},
   "source": [
    "Below we implement all of the incoming text into our pre process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(pred)):\n",
    "    pred[i]['Article'] = pre_process_text(pred[i]['Article'])\n",
    "    pred[i]\n",
    "    pred[i]['title'] = pre_process_text(pred[i]['title'].split(' '))\n",
    "pred[0]['title']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ea0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f98e1b",
   "metadata": {},
   "source": [
    "## Getting ticker/ signal data\n",
    "Below we will be getting the data to predict the data model.  First, as mentioned, we need to scrape for the article date.  I want to be able to tell in a 5 day span whether or not an article is good or bad.  Then based on the date, we can get a few signals based on the change in bar.  We will be looking for the variance in the market value that is:  the first market value of opening day minus the last market value of the closing day, minus the initial value.  \n",
    "\n",
    "Additionally, we can extrapolate the values instead of a magnitude, a simple buy/sell signal.  The former is much more precisise and could be highly valueable in production if results are acceptable.  The latter is much more common for sentiment analysis, and would be more likely to produce reliable results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a045057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the article timestamp\n",
    "        timestamp_element = soup.find('div', class_='caas-attr-time-style').find('time')\n",
    "\n",
    "        if timestamp_element is not None:\n",
    "            # Extract the timestamp text\n",
    "            timestamp = timestamp_element['datetime'].split('T')[0]\n",
    "            return timestamp\n",
    "        else:\n",
    "            return \"Timestamp not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f01a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets a signal if the stock went up or down in 1 or 7 days\n",
    "def get_signal(symbol, article_date, offset):\n",
    "    ad = datetime.strptime(article_date, '%Y-%m-%d')\n",
    "    barset = yf.download(symbol, start=ad, end=ad+ timedelta(days= offset), progress=False)\n",
    "    return (barset['Open'][0]-barset['Close'][-1]) / barset['Open'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fe104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_sell(val):\n",
    "    #Just need to see if a number is positive or negative: \n",
    "    if(val> 0):\n",
    "#        return \"Buy\" \n",
    "        return 1\n",
    "    if(val< 0):\n",
    "#        return \"Sell\" \n",
    "        return 0 \n",
    "    if(val == 0):\n",
    "#        return \"Hold\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d03ecf",
   "metadata": {},
   "source": [
    "Below we sample some time frames as responses to the article.  The origin date is when the article was published, the window length is the window that the market had to respond ot it. \n",
    "\n",
    "For this project we are lookinng at a 5 day timeframe.  If an article was dropped on a weekday, the prediction will be skipped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'https://finance.yahoo.com'\n",
    "resp = []\n",
    "for i in yahoo_finance_news:\n",
    "    date = get_datetime(root + i['link'])\n",
    "    if len(date) == 10: #Sloppy, because python does not lazy evaluate, and the second condition could cause an error.  \n",
    "        if datetime.strptime(get_datetime(root + i['link']), '%Y-%m-%d').weekday()  <5:  #Skipping weekdays.  Could look to next day.  \n",
    "            resp.append({'link' : i['link'], \"1 day\": get_signal(i['Symbol'], date, 1),\n",
    "                         \"1D Call\":buy_sell(get_signal(i['Symbol'], date, 1)),  \n",
    "                         \"2 day\": get_signal(i['Symbol'], date, 2),\"2D Call\":buy_sell(get_signal(i['Symbol'], date, 2)),  \n",
    "                         \"3 day\": get_signal(i['Symbol'], date, 3),\"3D Call\":buy_sell(get_signal(i['Symbol'], date, 3)),  \n",
    "                         \"4 day\": get_signal(i['Symbol'], date, 4),\"4D Call\":buy_sell(get_signal(i['Symbol'], date, 4)), \n",
    "                         \"5 day\": get_signal(i['Symbol'], date, 5), \"5D Call\":buy_sell(get_signal(i['Symbol'], date, 5))})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250e04b",
   "metadata": {},
   "source": [
    "* Sample of the response list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9ba81",
   "metadata": {},
   "source": [
    "## Assemble the data in a format that can run the data\n",
    "Below we join and convert the data into a single dataframe that is easy to read for both the eyes and the upcoming model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16857108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.merge(pd.DataFrame(pred), pd.DataFrame(resp), on='link', how = 'outer')\n",
    "df = df.dropna(subset = \"1 day\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = []\n",
    "for i in df.index:\n",
    "    tk = df['Article'][i] \n",
    "    rawtext = \"\"\n",
    "    for j in tk:\n",
    "        rawtext += j + \" \"\n",
    "    raw.append(rawtext)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3bad5",
   "metadata": {},
   "source": [
    "# Building the models\n",
    "As illuded to above, we want to experiment with different timeframes for when news affectuates into the market.  Each will reqire a different test/train set.  These are easy to set in scale, or manually.  For simplicity we will be adding 5 manually (less processing to build inline functions than recurring objects).  Sticking with the convention of splitting train and test data sets with an 80/20.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6157b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df[\"1 day\"]\n",
    "y2 = df[\"2 day\"]\n",
    "y3 = df[\"3 day\"]\n",
    "y4 = df[\"4 day\"]\n",
    "y5 = df[\"5 day\"]\n",
    "y1_train, y1_test = train_test_split(y1, test_size=0.2, random_state=42)\n",
    "y2_train, y2_test = train_test_split(y2, test_size=0.2, random_state=42)\n",
    "y3_train, y3_test = train_test_split(y3, test_size=0.2, random_state=42)\n",
    "y4_train, y4_test = train_test_split(y4, test_size=0.2, random_state=42)\n",
    "y5_train, y5_test = train_test_split(y5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baaf6c",
   "metadata": {},
   "source": [
    "## Vectorizing the article \n",
    "Below, we need to condition the predictor values for model evaluation.  For this roject we will be vectorizing the articles using Term Frequency - Inverse Documentation Frequency.  This transformer is prime for variable document lengths (blogs could be one or many paragraphs).  Additionally, they will be able to pick out spicific and important words like \"Cyber Attack\", \"Growth\", \"Beats\", or \"Dissapoints\".  As you can see the final output is an array of floats ranging from 0 to 1.  A few instances are in fact 0, which are likely 'useless' words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e764e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df['Raw'] \n",
    "X = df['Article']\n",
    "X_train_pre, X_test_pre = train_test_split(X, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "X_train = vectorizer.fit_transform(X_train_pre)\n",
    "X_train\n",
    "X_test = vectorizer.transform(X_test_pre)\n",
    "X_test.toarray()[0][:50]#Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934d3d8",
   "metadata": {},
   "source": [
    "## Adding the titles \n",
    "It may be advantagous to include the title, there may be precious words embedded within the title.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877422df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = df['title']+ df['Article']\n",
    "Xt_train_pre, Xt_test_pre = train_test_split(Xt, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "Xt_train = vectorizer.fit_transform(Xt_train_pre)\n",
    "Xt_train\n",
    "Xt_test = vectorizer.transform(Xt_test_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6bcd2",
   "metadata": {},
   "source": [
    "## Scale the different regressors\n",
    "Running multiple regressions takes up time and space.  It is best practice to build a single function that can run and analyze all of the functions.  This allows for simplicity and experimenation, particulary because I am running a total of 5 different responses.  Below, the model will be trained and evaluated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_regression(model_type, days = 5,_X_train =X_train,_X_test = X_test,  _y_train_list = [y1_train, y2_train, y3_train, y4_train, y5_train], y_test_list = [y1_test, y2_test, y3_test, y4_test, y5_test]):\n",
    "#Pre: You are giving the function a model type with .fit\n",
    "#Post: You get the predictions as well as the accuracy.  \n",
    "    y_pred_list = [] \n",
    "    rmse_list = []\n",
    "    for i in range(0,days-1): \n",
    "        model_type.fit(_X_train, _y_train_list[i])\n",
    "        y_pred_temp = model_type.predict(_X_test)\n",
    "        y_pred_list.append(y_pred_temp)\n",
    "        rmse_temp = np.sqrt(mean_squared_error(y_test_list[i], y_pred_temp))\n",
    "        rmse_list.append(rmse_temp)\n",
    "        print(i+1, \" Day:\")\n",
    "        print(\"Pred:  \", y_pred_temp[i])\n",
    "        print(\"Actual:\", sum(y_test_list[i])/ len(y_test_list[i]))\n",
    "        print(\"RMSE:  \", rmse_list[i])\n",
    "        print(\"MSE:   \", rmse_list[i] ** 2)\n",
    "    return rmse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285ba82",
   "metadata": {},
   "source": [
    "## Implement the regressors\n",
    "As you can see, even though I am using best practices and state of the art methods, I am still a long way from quitting my day job.  Most of the models appear to be unreliable and have more error than response.  It appears that the linear regression did the best thus far.  Linear regression is preferred because it adds an element of simplicity to a highly complex process and by far the least likely to cause any overfitting.  It would be wise to 'tighten up' the data model by fixing the hyper parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network preprocessing\n",
    "nn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input shape is determined by the number of features\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Convert the sparse matrix to a numpy array for training\n",
    "X_train_np = X_train.toarray()\n",
    "X_test_np = X_test.toarray()\n",
    "\n",
    "vocab_size = 10000  # Example value, adjust according to your data\n",
    "max_len = 100  # Example value, adjust according to your data\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
    "    LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "print(\"Linear Regression\")\n",
    "t1  = scale_regression(lr)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "print(\"\\nDecision Tree\")\n",
    "t1 = scale_regression(dt)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "#Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfg = RandomForestRegressor()\n",
    "print(\"\\nRandom Forest\")\n",
    "t1 = scale_regression(rfg)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "#SVR\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "print(\"\\nSRV\")\n",
    "t1 = scale_regression(svr)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "#Neural Network\n",
    "print(\"\\n Nerual Network\")\n",
    "t1 = scale_regression(nn, _X_train= X_train_np, _X_test= X_test_np, _y_train_list = [y1_train, y2_train, y3_train, y4_train, y5_train], y_test_list = [y1_test,  y2_test, y3_test, y4_test, y5_test])\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "#LSTM\n",
    "print(\"\\n LSTM\")\n",
    "t1 = scale_regression(lstm, _X_train= X_train_np, _X_test= X_test_np, _y_train_list = [y1_train, y2_train, y3_train, y4_train, y5_train], y_test_list = [y1_test,  y2_test, y3_test, y4_test, y5_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad0a07",
   "metadata": {},
   "source": [
    "## Implement the articles and titles for the possiblity of results.  \n",
    "It appears that adding the articles in fact helped!  This will be the ideal lead for looking for the best model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression\")\n",
    "t1 = scale_regression(lr, _X_train= Xt_train, _X_test= Xt_test)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "t1 = scale_regression(dt,  _X_train= Xt_train, _X_test= Xt_test)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "t1 = scale_regression(rfg, _X_train= Xt_train, _X_test= Xt_test)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n",
    "\n",
    "print(\"\\nSRV\")\n",
    "t1 = scale_regression(svr, _X_train= Xt_train, _X_test= Xt_test)\n",
    "print(\"Best Model: \", \"Day \", 1+ t1.index(min(t1)), \"RMSE:\", min(t1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44ae71",
   "metadata": {},
   "source": [
    "## Tune hyper parameters\n",
    "As illuded above, we must look for the best combination of variables to run thought the tree model.  It however appears that the inital model was a better fit, the tuning appears to have overtrained the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a32328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter grid\n",
    "param_dist = {\n",
    "    'criterion': ['mse', 'mae'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None] + list(range(5, 50, 5)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Create a Decision Tree Regressor model\n",
    "dt_model = DecisionTreeRegressor()\n",
    "\n",
    "# Use RandomizedSearchCV to tune hyperparameters\n",
    "random_search = RandomizedSearchCV(estimator=dt_model, param_distributions=param_dist, n_iter=100, cv=5, verbose=0, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y3_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_dt_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "test_score = best_dt_model.score(X_test, y3_test)\n",
    "print(\"Test Score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_best_pred = random_search.predict(X_test)\n",
    "best_mse = mean_squared_error(y3_test, y_best_pred)\n",
    "print(\"Best RMSE:\", np.sqrt(best_mse))\n",
    "print((sum(y3_test)/len(y3_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309315f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9d89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
